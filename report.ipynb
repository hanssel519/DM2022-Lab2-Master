{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cf74e6ba-448d-4f62-a58c-be4819a55ac7",
   "metadata": {},
   "source": [
    "Student Information\n",
    "Name: 朱育欣\n",
    "\n",
    "Student ID: 111062653\n",
    "\n",
    "GitHub ID: hanssel519\n",
    "\n",
    "Kaggle name: winnie hanssel chu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c8219-a487-421b-bba0-dadc92d547a9",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c27aca-356a-4f76-bf65-59289f78ae38",
   "metadata": {},
   "source": [
    "\n",
    "### Raw Data preprocess\n",
    "\n",
    "##### part2_preprocess.ipynb   (dump this part)\n",
    "1. convert the json raw data into pandas dataframe \n",
    "2. extract hashtags, tweet_id, text form \n",
    "3. concatenated the dataframe with the emotion label in the \n",
    "\n",
    "split the whole data set into train set and validation set (private_test).\n",
    "\n",
    "Text Preprocess\n",
    "delete the hashtag and username, alseo change the text to lowercase, and clean cleaning_punctuations, \n",
    "load \"glove.twitter.27B.100d.txt.gz\" with diemension = 100 as word2vec model\n",
    "\n",
    "TweetTokenizer\n",
    "\n",
    "Use nltk TweetTokenizer to tokenizer text, instead of usual tokenizer, since TweetTokenizer doesn't split token like @user or #hashtag into different tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65b02b7e-c435-44e2-9ab7-5bbd0e897785",
   "metadata": {},
   "source": [
    "The ranking at first was 95 and the accuracy was 0.22776, \n",
    "since I forgot to generate the test data to the same order of the \"sampleSubmission.csv\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3ae3f-ed7c-4571-87e9-1faef69f7468",
   "metadata": {},
   "source": [
    "### pretrained model for embeding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82132612-d56c-4475-8bed-aec9706bf55c",
   "metadata": {},
   "source": [
    "create embedding matrix with the pre-trained model: glove.twitter.27B.100d.txt.gz   \n",
    "and the tokenizer fit on \"train_df['p_text'].values\", (tokenizer.fit_on_texts(train_df['p_text'].values))   \n",
    "with the result of \"182765 unique tokens\"   \n",
    "also pad the converted sequence with length of 40   \n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "* row: vocab_size\n",
    "* column: dimension size (100)\n",
    "\n",
    "MAX_NB_WORDS = 60000\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "since glove.twitter.27B.100d has dimension of 100 for each word,  \n",
    "we set parameter EMBEDDING_DIM = 100\n",
    "\n",
    "* set lstm model with sigmoid activation function, adam as optimizer\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(hidden_nodes))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06dad86-8bf1-40fe-87a2-0a9c842f43a5",
   "metadata": {},
   "source": [
    "#### validation result (10% of training data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8dc171c-d494-490c-825c-5a2ec59e3e6b",
   "metadata": {},
   "source": [
    "y_prediction.shape:  (145557,)\n",
    "y_test_ground_truth.shape:  (145557,)\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       anger       0.75      0.09      0.16      3964\n",
    "anticipation       0.55      0.46      0.50     24975\n",
    "     disgust       0.34      0.20      0.25     13883\n",
    "        fear       0.60      0.18      0.28      6406\n",
    "         joy       0.48      0.79      0.60     51452\n",
    "     sadness       0.34      0.39      0.37     19284\n",
    "    surprise       0.97      0.09      0.17      4937\n",
    "       trust       0.55      0.20      0.30     20656\n",
    "\n",
    "    accuracy                           0.47    145557\n",
    "   macro avg       0.57      0.30      0.33    145557\n",
    "weighted avg       0.50      0.47      0.43    145557\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f750a-233a-4473-adb5-167aa8ec3928",
   "metadata": {},
   "source": [
    "#### test result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2b27c57-55ed-442f-8d5f-bf006a3a0708",
   "metadata": {},
   "source": [
    "Score: 0.408 (late submission time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3be1f-cbdb-449e-b725-16d42eadbcc3",
   "metadata": {},
   "source": [
    " -------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f7cbd-b426-4234-a576-c0b9750b2745",
   "metadata": {},
   "source": [
    "## with pre-trained model (cardiffnlp/twitter-roberta-base-emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5581e-233e-4231-b977-4f4d6af87508",
   "metadata": {},
   "source": [
    "#### training\n",
    "1. use OneHotEncoder to turn the 'emotion' feature into binary format  \n",
    "ex: ['anticipation']  -->  [0. 1. 0. 0. 0. 0. 0. 0.]  \n",
    "``` python\n",
    "label_data = np.array(train_dataset[\"emotion\"]).reshape(-1,1)\n",
    "encoder.fit(label_data)\n",
    "```\n",
    "2. use tokenizer from pretrained model to tokenize the 'text' feature  \n",
    "```python\n",
    "use tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "```\n",
    "3. function: preprocess, combine tokenized text with encoder.transform(emotion_list).toarray() as tokenized 'label' field\n",
    "4. split 20% train set as validation set\n",
    "5. create Trainer with parameters:\n",
    "    * LEARNING_RATE = 2e-5\n",
    "    * BATCH_SIZE =  8\n",
    "    * EPOCH = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410480b-e035-4f0d-8bc3-5270598deb69",
   "metadata": {},
   "source": [
    "#### evaluation, prediction\n",
    "\n",
    "1. trainer.predict(train_validation_dataset[\"test\"])\n",
    "2. chage type to tensor then .softmax(predictions, dim = -1)\n",
    "3. argmax from the 8 catagories\n",
    "4. one-hot for the inversion\n",
    "5. encoder.inverse back to original catagories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256e258-a5c2-49eb-8ee2-a6ac1ab7b8b3",
   "metadata": {},
   "source": [
    "#### test result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6941bb70-344f-497a-a980-8d4456993bce",
   "metadata": {},
   "source": [
    "Score: 0.56746 (late submission time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
